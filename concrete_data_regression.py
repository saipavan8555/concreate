# -*- coding: utf-8 -*-
"""concrete_data_regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dc3TNrYyBJsJvKXUcvNnv6d4V2G4X_OV
"""

5from google.colab import drive
drive .mount('/content/drive')

import os
os.chdir('/content/drive/My Drive/stastical_modelling')

import pandas as pd

sai = pd.read_csv('concrete_data.csv')

sai.head(12)

sai.tail(12)

sai.describe()

import plotly.express as py

first=py.scatter(sai,y='Fine_Aggregate',x='Concrete_compressive_strength')
first.show()

first=py.scatter(sai,x='Fine_Aggregate',y='Coarse_Aggregate')
first.show()

first=py.scatter(sai,x='Fine_Aggregate',y='Coarse_Aggregate')
first.show()

sai.corr()

sai.info()

sai.groupby('Age_days').aggregate(['mean','median','min','max'])['Concrete_compressive_strength']

import seaborn as sns

sns.pairplot(sai)

second=py.scatter(sai,x='Fine_Aggregate',y='Coarse_Aggregate',trendline="ols")
second.show()

sns.heatmap(sai.corr())

sns.heatmap(customers.corr(),annot=True,cmap='Greys')

second=py.scatter(sai,x='Age_days',y='Concrete_compressive_strength',trendline="ols")
second.show()

x=sai.drop(['Concrete_compressive_strength'],axis=1)

y=sai['Concrete_compressive_strength']

from sklearn.model_selection import train_test_split

from IPython.core.display import Image
Image('train_test_split',height=400,width=200)

import numpy as np

train_test_split(np.arange(1,21),test_size=0.3)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=101)

len(y_train)
print('Y Target Size:', len(y_train))
print('x Train Size:', len(x_train))
print('x Test  Size:', len(x_test))
print('Y Target Size:', len(y_test))

### without Random_state
sample = np.arange(1,11)
print(sample)

train_sample,test_sample = train_test_split(sample,test_size=0.2)
print('Train Sample data : ',train_sample)
print('Test Sample data : ',test_sample)

train_sample,test_sample = train_test_split(sample,test_size=0.2,random_state=3452)

print('Train Sample data : ',train_sample)
print('Test Sample data : ',test_sample)

import statsmodels.api as sm
from sklearn.model_selection import train_test_split

x_train_reg = sm.add_constant(x_train)
x_train_reg.head()

x_test_reg = sm.add_constant(x_test)
x_test_reg.head()

y_train[:5]

multiple_reg = sm.OLS(y_train, X_reg[['const', 'Avg. Session Length', 'Time on App', 'Time on Website']]).fit()

print(multiple_reg.summary())

multiple_reg = sm.OLS(y_train, x_train_reg[['const', 'Coarse_Aggregate', 'Fine_Aggregate', 'Blast_Furnace_Slag']]).fit() 

print(multiple_reg.summary())

train_dummy = pd.get_dummies(x_train_reg,drop_first=True,dtype='int8')
test_dummy = pd.get_dummies(x_test_reg,drop_first=True,dtype='int8')

train_dummy.head()

print(train_dummy.columns)
print("\n")
print(test_dummy.columns)

print("\n No. of columns in Train Data :{}".format(len(train_dummy.columns)))
print("\n")
print(test_dummy.columns)
print("\n No. of columns in Test Data :{}".format(len(test_dummy.columns)))

multiple_reg = sm.OLS(y_train, train_dummy).fit()

print(multiple_reg.summary())

from scipy import stats

def residual_plots(results):
    import matplotlib.pyplot as plt
    import statsmodels.formula.api as smf
    from statsmodels.nonparametric.smoothers_lowess import lowess
    from scipy import stats
    import statsmodels.api as sm
    
    
    fig, ax = plt.subplots(2,2,figsize=(14,10))
    ########## Residuals vs fitted  ##########
    residuals = results.resid
    fitted = results.fittedvalues
    smoothed = lowess(residuals,fitted)
    top3 = abs(residuals).sort_values(ascending = False)[:3]

    plt.rcParams.update({'font.size': 16})
    plt.rcParams["figure.figsize"] = (8,7)
    
    ax[0,0].scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')
    ax[0,0].plot(smoothed[:,0],smoothed[:,1],color = 'r')
    ax[0,0].set_ylabel('Residuals')
    ax[0,0].set_xlabel('Fitted Values')
    ax[0,0].set_title('Residuals vs. Fitted')
    ax[0,0].plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)

    for i in top3.index:
        ax[0,0].annotate(i,xy=(fitted[i],residuals[i]))

    #plt.show()
    
    
    ######### Normal qq plot ###########
    sorted_student_residuals = pd.Series(results.get_influence().resid_studentized_internal)
    sorted_student_residuals.index = results.resid.index
    sorted_student_residuals = sorted_student_residuals.sort_values(ascending = True)
    df = pd.DataFrame(sorted_student_residuals)
    df.columns = ['sorted_student_residuals']
    df['theoretical_quantiles'] = stats.probplot(df['sorted_student_residuals'], dist = 'norm', fit = False)[0]
    rankings = abs(df['sorted_student_residuals']).sort_values(ascending = False)
    top3 = rankings[:3]

    #fig, ax = plt.subplots()
    x = df['theoretical_quantiles']
    y = df['sorted_student_residuals']
    ax[0,1].scatter(x,y, edgecolor = 'k',facecolor = 'none')
    ax[0,1].set_title('Normal Q-Q')
    ax[0,1].set_ylabel('Standardized Residuals')
    ax[0,1].set_xlabel('Theoretical Quantiles')
    ax[0,1].plot([np.min([x,y]),np.max([x,y])],[np.min([x,y]),np.max([x,y])], color = 'r', ls = '--')
    for val in top3.index:
        ax[0,1].annotate(val,xy=(df['theoretical_quantiles'].loc[val],df['sorted_student_residuals'].loc[val]))
    #plt.show()
    
    
    ######### Homoscedasticity ############
    student_residuals = results.get_influence().resid_studentized_internal
    sqrt_student_residuals = pd.Series(np.sqrt(np.abs(student_residuals)))
    sqrt_student_residuals.index = results.resid.index
    smoothed = lowess(sqrt_student_residuals,fitted)
    top3 = abs(sqrt_student_residuals).sort_values(ascending = False)[:3]

    #fig, ax = plt.subplots()
    ax[1,0].scatter(fitted, sqrt_student_residuals, edgecolors = 'k', facecolors = 'none')
    ax[1,0].plot(smoothed[:,0],smoothed[:,1],color = 'r')
    ax[1,0].set_ylabel('$\sqrt{|Studentized \ Residuals|}$')
    ax[1,0].set_xlabel('Fitted Values')
    ax[1,0].set_title('Scale-Location')
    ax[1,0].set_ylim(0,max(sqrt_student_residuals)+0.1)
    for i in top3.index:
        ax[1,0].annotate(i,xy=(fitted[i],sqrt_student_residuals[i]))
    #plt.show()
    
    
    ######### Cooks distance  ###########
    student_residuals = pd.Series(results.get_influence().resid_studentized_internal)
    student_residuals.index = results.resid.index
    df = pd.DataFrame(student_residuals)
    df.columns = ['student_residuals']
    df['leverage'] = results.get_influence().hat_matrix_diag
    smoothed = lowess(df['student_residuals'],df['leverage'])
    sorted_student_residuals = abs(df['student_residuals']).sort_values(ascending = False)
    top3 = sorted_student_residuals[:3]

    #fig, ax = plt.subplots()
    x = df['leverage']
    y = df['student_residuals']
    xpos = max(x)+max(x)*0.01  
    ax[1,1].scatter(x, y, edgecolors = 'k', facecolors = 'none')
    ax[1,1].plot(smoothed[:,0],smoothed[:,1],color = 'r')
    ax[1,1].set_ylabel('Studentized Residuals')
    ax[1,1].set_xlabel('Leverage')
    ax[1,1].set_title('Residuals vs. Leverage')
    ax[1,1].set_ylim(min(y)-min(y)*0.15,max(y)+max(y)*0.15)
    ax[1,1].set_xlim(-0.01,max(x)+max(x)*0.05)
    plt.tight_layout()
    for val in top3.index:
        ax[1,1].annotate(val,xy=(x.loc[val],y.loc[val]))

    cooksx = np.linspace(min(x), xpos, 50)
    p = len(results.params)
    poscooks1y = np.sqrt((p*(1-cooksx))/cooksx)
    poscooks05y = np.sqrt(0.5*(p*(1-cooksx))/cooksx)
    negcooks1y = -np.sqrt((p*(1-cooksx))/cooksx)
    negcooks05y = -np.sqrt(0.5*(p*(1-cooksx))/cooksx)

    ax[1,1].plot(cooksx,poscooks1y,label = "Cook's Distance", ls = ':', color = 'r')
    ax[1,1].plot(cooksx,poscooks05y, ls = ':', color = 'r')
    ax[1,1].plot(cooksx,negcooks1y, ls = ':', color = 'r')
    ax[1,1].plot(cooksx,negcooks05y, ls = ':', color = 'r')
    ax[1,1].plot([0,0],ax[1,1].get_ylim(), ls=":", alpha = .3, color = 'k')
    ax[1,1].plot(ax[1,1].get_xlim(), [0,0], ls=":", alpha = .3, color = 'k')
    ax[1,1].annotate('1.0', xy = (xpos, poscooks1y[-1]), color = 'r')
    ax[1,1].annotate('0.5', xy = (xpos, poscooks05y[-1]), color = 'r')
    ax[1,1].annotate('1.0', xy = (xpos, negcooks1y[-1]), color = 'r')
    ax[1,1].annotate('0.5', xy = (xpos, negcooks05y[-1]), color = 'r')
    ax[1,1].legend()
    plt.show()

residual_plots(multiple_reg)

from statsmodels.stats.outliers_influence import OLSInfluence

influence_points = OLSInfluence(multiple_reg)
pd.DataFrame(influence_points.influence).describe()

sai_clean = sai.drop(columns = ['Cement','Fine_Aggregate'],axis=1)

influence_data = pd.DataFrame(influence_points.influence[influence_points.influence>=1])
sai_clean['influence_value']= influence_points.influence
sai_clean.loc[influence_data.index,].sort_values(['influence_value'], ascending = False).head(10)

import matplotlib.pyplot as plt

from statsmodels.graphics.regressionplots import plot_leverage_resid2
fig, ax = plt.subplots(figsize=(8, 6))
fig = plot_leverage_resid2(multiple_reg, ax=ax)

sai_clean.loc[[228,66,383]]

from sklearn.preprocessing import MinMaxScaler, StandardScaler
scaler = StandardScaler()

num_vars = ['Water', 'Coarse_Aggregate', 'Fine_Aggregate',
       'Age_days']

X = sai_clean.drop(['Concrete_compressive_strength','influence_value'],axis=1)

y = sai_clean['Concrete_compressive_strength']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

X_train.head()

x_train[num_vars] = scaler.fit_transform(x_train[num_vars])
x_test[num_vars] = scaler.fit_transform(x_test[num_vars])

# adding the Constant term
X_train = sm.add_constant(X_train)
print(X_train.head())

X_test = sm.add_constant(X_test)
#X_test.head()

scaler = StandardScaler()
x_train[num_vars] = scaler.fit_transform(x_train[num_vars])
x_test[num_vars] = scaler.fit_transform(x_test[num_vars])

print(x_train.head())
# adding the Constant term
x_trainReg = sm.add_constant(x_train)
x_testReg = sm.add_constant(x_test)
print(x_trainReg.head())
x_trainReg = pd.get_dummies(x_trainReg,drop_first=True,dtype='int8')
x_testReg = pd.get_dummies(x_testReg,drop_first=True,dtype='int8')
print(x_trainReg.head())

y_train[:10]

x_trainReg[:10]

model = sm.OLS(y_train, x_trainReg)
results = model.fit()
print(results.summary())

residual_plots(results)

from sklearn.linear_model import LinearRegression
lm = LinearRegression()

lm.fit(x_train,y_train)

!pip install mlxtend

import joblib
import sys
sys.modules['sklearn.externals.joblib'] = joblib
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

sfs = SFS(lm,
         k_features='best',
         forward=True,
         floating=False,
         scoring='r2',
         cv=5)

sfs = sfs.fit(X_train,y_train)

sfs_results = pd.DataFrame.from_dict(sfs.get_metric_dict(confidence_interval=0.95)).T
sfs_results

plot_sfs(sfs.get_metric_dict(), kind='std_err')
plt.grid()
plt.show()

from statsmodels.stats.outliers_influence import variance_inflation_factor



sns.heatmap(sai_clean.drop(['influence_value'],axis=1).corr(),annot=True,cmap='Greens')

from sklearn.feature_selection import RFE

indp_vars = X
min_aic = 10**100 # initiailizing AIC to a very large value 
for i in range(indp_vars.shape[1],0,-1):
    regression_model = LinearRegression()
    rfe = RFE(regression_model, n_features_to_select=i)
    rfe = rfe.fit(X_train, y_train)
    chk = X_train.loc[:,list(rfe.support_)]
    print(chk.columns)
    regression_model=sm.OLS(y_train,chk)
    result=regression_model.fit()
    current_aic = result.aic
    print(current_aic)
    if(current_aic <= min_aic):
        min_aic = current_aic
        bestmodel = chk.columns
    else:
        print("Break !")
        break

bestmodel_data = X_train.loc[:,bestmodel]
bestmodel_data['intercept'] =1
bestmodel_data.head()
regression_model=sm.OLS(y_train,bestmodel_data)
result=regression_model.fit()
print(result.summary2())

